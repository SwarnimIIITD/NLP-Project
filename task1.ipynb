{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7df5d9-2479-4189-bd34-e61179eea8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN + Glove\n",
      "f1 0.8147603008421049\n",
      "f1_chunk 0.6811704213688271\n",
      "f1_tag 0.9483501803153829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU + Glove\n",
      "f1 0.8137980366707451\n",
      "f1_chunk 0.6794268561818558\n",
      "f1_tag 0.9481692171596341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU + Fasttext\n",
      "f1 0.7836185498143601\n",
      "f1_chunk 0.6240937762144577\n",
      "f1_tag 0.9431433234142622\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import gensim.downloader as api\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from conlleval import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_to_index = {'B':0, 'I':1, 'O':2}\n",
    "def preprocess_data(data):\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        aspects = []\n",
    "        for aspect in item['aspect_terms']:\n",
    "            aspects.append(aspect['term'])\n",
    "\n",
    "        sentence = item['sentence']\n",
    "        tokens = word_tokenize(sentence)\n",
    "        length = len(tokens)\n",
    "        labels = ['O'] * length\n",
    "\n",
    "        for aspect in item['aspect_terms']:\n",
    "            start_index = int(aspect['from'])\n",
    "            end_index = int(aspect['to'])\n",
    "            aspect_part = sentence[start_index:end_index+1]\n",
    "            aspect_tokens = word_tokenize(aspect_part)\n",
    "            aspect_length = len(aspect_tokens)\n",
    "\n",
    "            for i in range(length):\n",
    "                if tokens[i] == aspect_tokens[0]:\n",
    "                    labels[i] = 'B'\n",
    "                    for j in range(aspect_length-1):\n",
    "                        labels[i+j+1] = 'I'\n",
    "                    break\n",
    "\n",
    "        preprocessed_data .append(\n",
    "            {\n",
    "                'sentence' : item['sentence'],\n",
    "                'tokens' : tokens,\n",
    "                'labels' : labels,\n",
    "                'aspect_terms' : aspects,\n",
    "            }\n",
    "        )\n",
    "    return preprocessed_data\n",
    "\n",
    "def get_embedding(data,embedding_model):\n",
    "    embedded_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        labels = item['labels']\n",
    "        word_embedding = []\n",
    "        new_labels = []\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in embedding_model:\n",
    "                word_embedding.append(embedding_model[tokens[i]])\n",
    "                new_labels.append(labels[i])\n",
    "                \n",
    "        embedded_data.append(\n",
    "            {\n",
    "            'sentence' : item['sentence'],\n",
    "            'tokens' : tokens,\n",
    "            'embeddings' : word_embedding,\n",
    "            'labels' : new_labels,\n",
    "            'aspect_terms' : item['aspect_terms']\n",
    "            }\n",
    "        )\n",
    "    return embedded_data\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        item = self.data[idx]\n",
    "        embeddings = np.array(item['embeddings'], dtype=np.float32)\n",
    "        labels = np.array(item['labels'], dtype=np.int64)\n",
    "        labels = labels[:len(embeddings)]\n",
    "        return torch.tensor(embeddings), torch.tensor(labels)\n",
    "\n",
    "def label_indexing(data, label_to_index):\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        \n",
    "        label_index = []\n",
    "        for label in item['labels']:\n",
    "            label_index.append(label_to_index[label])\n",
    "\n",
    "\n",
    "        new_entry = {\n",
    "            'sentence': item['sentence'],\n",
    "            'tokens': item['tokens'],\n",
    "            'labels': label_index,\n",
    "            'aspect_terms': item['aspect_terms']\n",
    "        }\n",
    "        new_data.append(new_entry)\n",
    "\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device) \n",
    "\n",
    "        out, _ = self.gru(x, h0)  \n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc_out(out)  \n",
    "        return out\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  \n",
    "\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "\n",
    "       \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc_out(out)  \n",
    "        return out\n",
    "        \n",
    "lr = 0.001\n",
    "\n",
    "def evaluate_test_data(best_model_path, test_file_path,flag):\n",
    "    with open(test_file_path, 'r') as file:\n",
    "        test_data = json.load(file)\n",
    "\n",
    "    test_data_processed = preprocess_data(test_data)\n",
    "    test = label_indexing(test_data_processed,label_to_index)\n",
    "    \n",
    "    best_model = torch.load('C:/Users/rites/Downloads/best_model.pt', weights_only=False)\n",
    "\n",
    "    def format_for_conlleval(labels, predictions):\n",
    "        return [f\"w{idx} O {true} {pred}\" for idx, (true, pred) in enumerate(zip(labels, predictions))]\n",
    "        \n",
    "    if flag == True:\n",
    "        glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "        \n",
    "        test1 = get_embedding(test, glove_model)\n",
    "        \n",
    "        print(test1)\n",
    "    \n",
    "        test_dataset = custom_dataset(test1)\n",
    "    \n",
    "        print(test_dataset)\n",
    "\n",
    "        max1 = 0\n",
    "        for item in test_dataset:\n",
    "            max1 = max(len(item[0]),max1)\n",
    "        \n",
    "        new_test_dataset = []\n",
    "    \n",
    "        for item in test_dataset:\n",
    "    \n",
    "            \n",
    "            zero_rows = torch.zeros((max1 - len(item[0]), 300))  \n",
    "            padded_embeddings = torch.cat((item[0], zero_rows), dim=0)\n",
    "        \n",
    "            padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "        \n",
    "            new_test_dataset.append((padded_embeddings, padded_labels))\n",
    "    \n",
    "        test_dataset = new_test_dataset\n",
    "        \n",
    "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "        \n",
    "        best_model.eval()  \n",
    "        overall_test_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            for embeddings, labels in test_loader:\n",
    "                outputs = best_model(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                overall_test_loss += loss\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "                \n",
    "        print(f1_sum/len(test_loader))\n",
    "        \n",
    "    if flag == False:\n",
    "        glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "    \n",
    "        test1 = get_embedding(test, glove_model)\n",
    "        \n",
    "        print(test1)\n",
    "    \n",
    "        test_dataset = custom_dataset(test1)\n",
    "    \n",
    "        print(test_dataset)\n",
    "\n",
    "        max1 = 0\n",
    "        for item in test_dataset:\n",
    "            max1 = max(len(item[0]),max1)\n",
    "        \n",
    "        new_test_dataset = []\n",
    "    \n",
    "        for item in test_dataset:\n",
    "            length = len(item[0])\n",
    "            \n",
    "            \n",
    "            zero_rows = torch.zeros((max1 - length, 300))  \n",
    "            padded_embeddings = torch.cat((item[0], zero_rows), dim=0)\n",
    "        \n",
    "            # Pad labels with -1\n",
    "            padded_labels = torch.cat((item[1], torch.full((max1 - length,), -1, dtype=torch.long)))\n",
    "        \n",
    "            new_test_dataset.append((padded_embeddings, padded_labels))\n",
    "    \n",
    "        test_dataset = new_test_dataset\n",
    "        \n",
    "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "        best_model.eval()  \n",
    "        overall_test_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            for embeddings, labels in test_loader:\n",
    "                outputs = best_model(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                overall_val_loss += loss\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "                \n",
    "        print(f1_sum/len(test_loader))\n",
    "    \n",
    "flag = False\n",
    "epoch_cycle = [1,2,3,4,5]\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    with open('C:/Users/rites/Downloads/train.json', 'r') as file:\n",
    "        train_data = json.load(file)\n",
    "    \n",
    "    with open('C:/Users/rites/Downloads/val.json', 'r') as file:\n",
    "        val_data = json.load(file)\n",
    "\n",
    "    with open('C:/Users/rites/Downloads/train_task_1.json', 'w') as file:\n",
    "        json.dump(preprocess_data(train_data),file)\n",
    "\n",
    "    with open('C:/Users/rites/Downloads/val_task_1.json', 'w') as file:\n",
    "        json.dump(preprocess_data(val_data),file)\n",
    "\n",
    "    with open('C:/Users/rites/Downloads/train_task_1.json', 'r') as file:\n",
    "        train_data_processed = json.load(file)\n",
    "    \n",
    "    with open('C:/Users/rites/Downloads/val_task_1.json', 'r') as file:\n",
    "        val_data_processed = json.load(file)\n",
    "        \n",
    "    train = label_indexing(train_data_processed,label_to_index)\n",
    "    val = label_indexing(val_data_processed,label_to_index)\n",
    "\n",
    "    # model 1 -> rnn + glove\n",
    "    glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "    \n",
    "    train1 = get_embedding(train, glove_model)\n",
    "    val1 = get_embedding(val,glove_model)\n",
    "    \n",
    "    print(train1)\n",
    "    print(val1)\n",
    "\n",
    "    train_dataset = custom_dataset(train1)\n",
    "    val_dataset = custom_dataset(val1)\n",
    "\n",
    "    print(train_dataset)\n",
    "    print(val_dataset)\n",
    "\n",
    "    max1 = 0\n",
    "    for item in train_dataset:\n",
    "        max1 = max(len(item[0]),max1)\n",
    "    \n",
    "    new_train_dataset = []\n",
    "\n",
    "    for item in train_dataset:\n",
    "     \n",
    "        \n",
    "       \n",
    "\n",
    "        padded_embeddings = torch.cat((item[0], torch.zeros((max1 - len(item[0]), 300)) ), dim=0)\n",
    "    \n",
    "        \n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_train_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    train_dataset = new_train_dataset\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "    \n",
    "    max1 = 0\n",
    "    for item in val_dataset:\n",
    "        max1 = max(len(item[0]),max1)\n",
    "    \n",
    "    new_val_dataset = []\n",
    "\n",
    "    for item in val_dataset:\n",
    "    \n",
    "        \n",
    "     \n",
    "        padded_embeddings = torch.cat((item[0], torch.zeros((max1 - len(item[0]), 300)) ), dim=0)\n",
    "    \n",
    "      \n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_val_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    val_dataset = new_val_dataset\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    rnn_model1 = RNNModel(input_dim=300, hidden_dim=128, output_dim=3)\n",
    "\n",
    "    optimizer = optim.Adam(rnn_model1.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1) \n",
    "    \n",
    "    f1_score_models = []\n",
    "    f1_chunk_score_models = []\n",
    "    f1_tag_score_models = []\n",
    "    num_epochs = 5\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    epoch_f1 = []\n",
    "    epoch_f1_chunk = []\n",
    "    epoch_f1_tag = []\n",
    "    idx_to_bio = {0: 'B', 1: 'I', 2: 'O'}\n",
    "\n",
    "    \n",
    "    \n",
    "    def format_for_conlleval(labels, predictions):\n",
    "        return [f\"w{idx} O {true} {pred}\" for idx, (true, pred) in enumerate(zip(labels, predictions))]\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        rnn_model1.train()\n",
    "        overall_train_loss = 0\n",
    "        for embeddings, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = rnn_model1(embeddings)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            overall_train_loss += loss.item()\n",
    "            \n",
    "        train_loss.append(overall_train_loss/len(train_loader))\n",
    "\n",
    "        rnn_model1.eval()  \n",
    "        overall_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            f1_sum_chunk = 0\n",
    "            f1_sum_tag = 0\n",
    "            for embeddings, labels in val_loader:\n",
    "                outputs = rnn_model1(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                overall_val_loss += loss.item()\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "                f1_sum_chunk += chunk_f1\n",
    "                f1_sum_tag += tag_f1\n",
    "                \n",
    "        epoch_f1_chunk.append(f1_sum_chunk/len(val_loader))\n",
    "        epoch_f1_tag.append(f1_sum_tag/len(val_loader))        \n",
    "        epoch_f1.append(f1_sum/len(val_loader))\n",
    "        \n",
    "        val_loss.append(overall_val_loss/len(val_loader))\n",
    "\n",
    "    print(\"RNN + Glove\")\n",
    "    print(\"f1\",np.mean(epoch_f1))\n",
    "    print(\"f1_chunk\",np.mean(epoch_f1_chunk))\n",
    "    print(\"f1_tag\",np.mean(epoch_f1_tag))\n",
    "    \n",
    "    f1_score_models.append(np.mean(epoch_f1))\n",
    "    f1_chunk_score_models.append(np.mean(epoch_f1_chunk))\n",
    "    f1_tag_score_models.append(np.mean(epoch_f1_tag))\n",
    "    \n",
    "    plt.plot(epoch_cycle, val_loss)\n",
    "    plt.plot(epoch_cycle,train_loss)\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('losses')\n",
    "    \n",
    "    plt.savefig('plot1.png') \n",
    "    plt.close()\n",
    "    \n",
    "    # model2 -> gru + glove\n",
    "    glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "    \n",
    "    train1 = get_embedding(train, glove_model)\n",
    "    val1 = get_embedding(val,glove_model)\n",
    "    \n",
    "    print(train1)\n",
    "    print(val1)\n",
    "\n",
    "    train_dataset = custom_dataset(train1)\n",
    "    val_dataset = custom_dataset(val1)\n",
    "\n",
    "    print(train_dataset)\n",
    "    print(val_dataset)\n",
    "\n",
    "    max1 = 0\n",
    "    for item in train_dataset:\n",
    "        max1 = max(len(item[0]),max1)\n",
    "\n",
    "    new_train_dataset = []\n",
    "    \n",
    "    for item in train_dataset:\n",
    "        \n",
    "        padded_embeddings = torch.cat((item[0],  torch.zeros((max1 - len(item[0]), 300))), dim=0)\n",
    "    \n",
    "        \n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_train_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    train_dataset = new_train_dataset\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "    new_val_dataset = []\n",
    "\n",
    "    for item in val_dataset:\n",
    "        \n",
    "\n",
    "        zero_rows = torch.zeros((max1 - len(item[0]), 300))  \n",
    "        padded_embeddings = torch.cat((item[0], zero_rows), dim=0)\n",
    "    \n",
    "        \n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_val_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    val_dataset = new_val_dataset\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    gru_model1 = GRUModel(input_dim=300, hidden_dim=128, output_dim=3)\n",
    "\n",
    "    optimizer = optim.Adam(gru_model1.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1) \n",
    "\n",
    "    num_epochs = 5\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    epoch_f1 = []\n",
    "    epoch_f1_tag = []\n",
    "    epoch_f1_chunk = []\n",
    "    for epoch in range(num_epochs):\n",
    "        gru_model1.train()\n",
    "        overall_train_loss = 0\n",
    "        for embeddings, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = gru_model1(embeddings)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            overall_train_loss += loss.item()\n",
    "            \n",
    "        train_loss.append(overall_train_loss/len(train_loader))\n",
    "\n",
    "        gru_model1.eval()  \n",
    "        overall_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            f1_sum_chunk = 0\n",
    "            f1_sum_tag = 0\n",
    "            for embeddings, labels in val_loader:\n",
    "                outputs = gru_model1(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                overall_val_loss += loss.item()\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "                f1_sum_chunk += chunk_f1\n",
    "                f1_sum_tag += tag_f1\n",
    "                \n",
    "        epoch_f1_chunk.append(f1_sum_chunk/len(val_loader))\n",
    "        epoch_f1_tag.append(f1_sum_tag/len(val_loader))\n",
    "        epoch_f1.append(f1_sum/len(val_loader))\n",
    "        \n",
    "        val_loss.append(overall_val_loss/len(val_loader))\n",
    "\n",
    "    plt.plot(epoch_cycle, val_loss)\n",
    "    plt.plot(epoch_cycle,train_loss)\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('losses')\n",
    "    \n",
    "    plt.savefig('plot2.png') \n",
    "    plt.close()\n",
    "    \n",
    "    print(\"GRU + Glove\")\n",
    "    print(\"f1\",np.mean(epoch_f1))\n",
    "    print(\"f1_chunk\",np.mean(epoch_f1_chunk))\n",
    "    print(\"f1_tag\",np.mean(epoch_f1_tag))\n",
    "    \n",
    "    f1_score_models.append(np.mean(epoch_f1))\n",
    "    f1_chunk_score_models.append(np.mean(epoch_f1_chunk))\n",
    "    f1_tag_score_models.append(np.mean(epoch_f1_tag))\n",
    "\n",
    "    fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "    \n",
    "    train1 = get_embedding(train, fasttext_model)\n",
    "    val1 = get_embedding(val,fasttext_model)\n",
    "    \n",
    "    print(train1)\n",
    "    print(val1)\n",
    "\n",
    "    train_dataset = custom_dataset(train1)\n",
    "    val_dataset = custom_dataset(val1)\n",
    "\n",
    "    print(train_dataset)\n",
    "    print(val_dataset)\n",
    "\n",
    "    max1 = 0\n",
    "    for item in train_dataset:\n",
    "        max1 = max(len(item[0]),max1)\n",
    "\n",
    "    new_train_dataset = []\n",
    "    \n",
    "    for item in train_dataset:\n",
    "       \n",
    "        padded_embeddings = torch.cat((item[0], torch.zeros((max1 - len(item[0]), 300))  ), dim=0)\n",
    "    \n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_train_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    train_dataset = new_train_dataset\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "    new_val_dataset = []\n",
    "\n",
    "    for item in val_dataset:\n",
    "     \n",
    "       \n",
    "        \n",
    "        padded_embeddings = torch.cat((item[0], torch.zeros((max1 -  len(item[0]), 300))  ), dim=0)\n",
    "    \n",
    "        # Pad labels with -1\n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 -  len(item[0]),), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_val_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    val_dataset = new_val_dataset\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    gru_model2 = GRUModel(input_dim=300, hidden_dim=128, output_dim=3)\n",
    "\n",
    "    optimizer = optim.Adam(gru_model2.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1) \n",
    "\n",
    "    num_epochs = 5\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    epoch_f1 = []\n",
    "    epoch_f1_tag = []\n",
    "    epoch_f1_chunk = []\n",
    "    for epoch in range(num_epochs):\n",
    "        gru_model2.train()\n",
    "        overall_train_loss = 0\n",
    "        for embeddings, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = gru_model2(embeddings)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            overall_train_loss += loss.item()\n",
    "            \n",
    "        train_loss.append(overall_train_loss/len(train_loader))\n",
    "\n",
    "        gru_model2.eval()  \n",
    "        overall_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            f1_sum_chunk = 0\n",
    "            f1_sum_tag = 0\n",
    "            for embeddings, labels in val_loader:\n",
    "                outputs = gru_model2(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                overall_val_loss += loss.item()\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "                f1_sum_chunk += chunk_f1\n",
    "                f1_sum_tag += tag_f1\n",
    "                \n",
    "        epoch_f1_chunk.append(f1_sum_chunk/len(val_loader))\n",
    "        epoch_f1_tag.append(f1_sum_tag/len(val_loader))\n",
    "        epoch_f1.append(f1_sum/len(val_loader))\n",
    "        \n",
    "        val_loss.append(overall_val_loss/len(val_loader))\n",
    "\n",
    "    print(\"GRU + Fasttext\")\n",
    "    print(\"f1\",np.mean(epoch_f1))\n",
    "    print(\"f1_chunk\",np.mean(epoch_f1_chunk))\n",
    "    print(\"f1_tag\",np.mean(epoch_f1_tag))\n",
    "    \n",
    "    f1_score_models.append(np.mean(epoch_f1))\n",
    "    f1_chunk_score_models.append(np.mean(epoch_f1_chunk))\n",
    "    f1_tag_score_models.append(np.mean(epoch_f1_tag))\n",
    "    \n",
    "    plt.plot(epoch_cycle, val_loss)\n",
    "    plt.plot(epoch_cycle,train_loss)\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('losses')\n",
    "    \n",
    "    plt.savefig('plot3.png') \n",
    "    plt.close()\n",
    "    \n",
    "    # model 4 -> rnn + fastext\n",
    "    fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "    \n",
    "    train1 = get_embedding(train, fasttext_model)\n",
    "    val1 = get_embedding(val,fasttext_model)\n",
    "    \n",
    "    print(train1)\n",
    "    print(val1)\n",
    "\n",
    "    train_dataset = custom_dataset(train1)\n",
    "    val_dataset = custom_dataset(val1)\n",
    "\n",
    "    print(train_dataset)\n",
    "    print(val_dataset)\n",
    "\n",
    "    max1 = 0\n",
    "    for item in train_dataset:\n",
    "        max1 = max(len(item[0]),max1)\n",
    "\n",
    "    new_train_dataset = []\n",
    "    \n",
    "    for item in train_dataset:\n",
    "       \n",
    "        padded_embeddings = torch.cat((item[0], torch.zeros((max1 - len(item[0]), 300))), dim=0)\n",
    "    \n",
    "       \n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_train_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    train_dataset = new_train_dataset\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "    new_val_dataset = []\n",
    "\n",
    "    for item in val_dataset:\n",
    "        length = len(item[0])\n",
    "        \n",
    "        zero_rows = torch.zeros((max1 - length, 300))  \n",
    "        padded_embeddings = torch.cat((item[0], zero_rows), dim=0)\n",
    "    \n",
    "        padded_labels = torch.cat((item[1], torch.full((max1 - length,), -1, dtype=torch.long)))\n",
    "    \n",
    "        new_val_dataset.append((padded_embeddings, padded_labels))\n",
    "\n",
    "    val_dataset = new_val_dataset\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    rnn_model2 = RNNModel(input_dim=300, hidden_dim=128, output_dim=3)\n",
    "\n",
    "    optimizer = optim.Adam(rnn_model2.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1) \n",
    "\n",
    "    num_epochs = 5\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    epoch_f1 = []\n",
    "    epoch_f1_tag = []\n",
    "    epoch_f1_chunk = []\n",
    "    for epoch in range(num_epochs):\n",
    "        rnn_model2.train()\n",
    "        overall_train_loss = 0\n",
    "        for embeddings, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = rnn_model2(embeddings)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            overall_train_loss += loss.item()\n",
    "            \n",
    "        train_loss.append(overall_train_loss/len(train_loader))\n",
    "\n",
    "        rnn_model2.eval()  \n",
    "        overall_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            f1_sum_chunk = 0\n",
    "            f1_sum_tag = 0\n",
    "            for embeddings, labels in val_loader:\n",
    "                outputs = rnn_model2(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                overall_val_loss += loss.item()\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "                f1_sum_chunk += chunk_f1\n",
    "                f1_sum_tag += tag_f1\n",
    "                \n",
    "        epoch_f1_chunk.append(f1_sum_chunk/len(val_loader))\n",
    "        epoch_f1_tag.append(f1_sum_tag/len(val_loader))\n",
    "        epoch_f1.append(f1_sum/len(val_loader))\n",
    "        \n",
    "        val_loss.append(overall_val_loss/len(val_loader))\n",
    "\n",
    "    print(\"RNN + Fasttext\")\n",
    "    print(\"f1\",np.mean(epoch_f1))\n",
    "    print(\"f1_chunk\",np.mean(epoch_f1_chunk))\n",
    "    print(\"f1_tag\",np.mean(epoch_f1_tag))\n",
    "    \n",
    "    f1_score_models.append(np.mean(epoch_f1))\n",
    "    f1_chunk_score_models.append(np.mean(epoch_f1_chunk))\n",
    "    f1_tag_score_models.append(np.mean(epoch_f1_tag))\n",
    "    \n",
    "    plt.plot(epoch_cycle, val_loss)\n",
    "    plt.plot(epoch_cycle,train_loss)\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('losses')\n",
    "    \n",
    "    plt.savefig('plot4.png') \n",
    "    plt.close()\n",
    "    \n",
    "    max_index = f1_score_models.index(max(f1_score_models)) \n",
    "\n",
    "    if max_index == 0:\n",
    "        torch.save(rnn_model1, 'C:/Users/rites/Downloads/best_model.pt') \n",
    "        print(\"f1_score_models\",f1_score_models[max_index])\n",
    "        print(\"f1_chunk_score_models\",f1_chunk_score_models[max_index])\n",
    "        print(\"f1_tag_score_models\",f1_tag_score_models[max_index])\n",
    "        print(\"rnn + glove\")\n",
    "        flag = True\n",
    "    if max_index == 1:\n",
    "        torch.save(gru_model1,'C:/Users/rites/Downloads/best_model.pt') \n",
    "        print(\"f1_score_models\",f1_score_models[max_index])\n",
    "        print(\"f1_chunk_score_models\",f1_chunk_score_models[max_index])\n",
    "        print(\"f1_tag_score_models\",f1_tag_score_models[max_index])\n",
    "        print(\"gru + glove\")\n",
    "        flag = True\n",
    "    if max_index == 2:\n",
    "        torch.save(gru_model2,'C:/Users/rites/Downloads/best_model.pt') \n",
    "        print(\"f1_score_models\",f1_score_models[max_index])\n",
    "        print(\"f1_chunk_score_models\",f1_chunk_score_models[max_index])\n",
    "        print(\"f1_tag_score_models\",f1_tag_score_models[max_index])\n",
    "        print(\"gru + fasttext\")\n",
    "    if max_index == 3:\n",
    "        torch.save(rnn_model2,'C:/Users/rites/Downloads/best_model.pt') \n",
    "        print(\"f1_score_models\",f1_score_models[max_index])\n",
    "        print(\"f1_chunk_score_models\",f1_chunk_score_models[max_index])\n",
    "        print(\"f1_tag_score_models\",f1_tag_score_models[max_index])\n",
    "        print(\"rnn + fasttext\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea43568-76dc-41cd-a03f-5eb145ef9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032098687081717994\n",
      "0.8336598576760306\n",
      "0.43287927237887425\n"
     ]
    }
   ],
   "source": [
    "evaluate_test_data('C:/Users/rites/Downloads/best_model.pt', 'C:/Users/rites/Downloads/test.json',flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf268c7d-39f9-438a-aec0-f77be4811731",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_bio = {0: 'B', 1: 'I', 2: 'O'}\n",
    "def evaluate_test_data(best_model_path, test_file_path,flag):\n",
    "    with open(test_file_path, 'r') as file:\n",
    "        test_data = json.load(file)\n",
    "\n",
    "    test_data_processed = preprocess_data(test_data)\n",
    "    test = label_indexing(test_data_processed,label_to_index)\n",
    "    \n",
    "    best_model = torch.load(best_model_path, weights_only=False)\n",
    "\n",
    "    def format_for_conlleval(labels, predictions):\n",
    "        return [f\"w{idx} O {true} {pred}\" for idx, (true, pred) in enumerate(zip(labels, predictions))]\n",
    "\n",
    "        \n",
    "    if flag == True:\n",
    "        glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "        \n",
    "        test1 = get_embedding(test, glove_model)\n",
    "        \n",
    "        print(test1)\n",
    "    \n",
    "        test_dataset = custom_dataset(test1)\n",
    "    \n",
    "        print(test_dataset)\n",
    "\n",
    "        max1 = 0\n",
    "        for item in test_dataset:\n",
    "            max1 = max(len(item[0]),max1)\n",
    "        \n",
    "        new_test_dataset = []\n",
    "    \n",
    "        for item in test_dataset:\n",
    "            \n",
    "\n",
    "            padded_embeddings = torch.cat((item[0], torch.zeros((max1 - len(item[0]), 300))  ), dim=0)\n",
    "        \n",
    "            padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "        \n",
    "            new_test_dataset.append((padded_embeddings, padded_labels))\n",
    "    \n",
    "        test_dataset = new_test_dataset\n",
    "        \n",
    "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "        \n",
    "        best_model.eval()  \n",
    "        overall_test_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            f1_sum_chunk = 0\n",
    "            f1_sum_tag = 0\n",
    "            for embeddings, labels in test_loader:\n",
    "                outputs = best_model(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum_chunk += chunk_f1\n",
    "                f1_sum_tag += tag_f1\n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "\n",
    "        print(f1_sum_chunk/len(test_loader))\n",
    "        print(f1_sum_tag/len(test_loader))\n",
    "        print(f1_sum/len(test_loader))\n",
    "        \n",
    "    if flag == False:\n",
    "        glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "    \n",
    "        test1 = get_embedding(test, glove_model)\n",
    "        \n",
    "        print(test1)\n",
    "    \n",
    "        test_dataset = custom_dataset(test1)\n",
    "    \n",
    "        print(test_dataset)\n",
    "\n",
    "        max1 = 0\n",
    "        for item in test_dataset:\n",
    "            max1 = max(len(item[0]),max1)\n",
    "        \n",
    "        new_test_dataset = []\n",
    "    \n",
    "        for item in test_dataset:\n",
    "      \n",
    "\n",
    "            padded_embeddings = torch.cat((item[0], torch.zeros((max1 - len(item[0]), 300))  ), dim=0)\n",
    "        \n",
    "          \n",
    "            padded_labels = torch.cat((item[1], torch.full((max1 - len(item[0]),), -1, dtype=torch.long)))\n",
    "        \n",
    "            new_test_dataset.append((padded_embeddings, padded_labels))\n",
    "    \n",
    "        test_dataset = new_test_dataset\n",
    "        \n",
    "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "        best_model.eval()  \n",
    "        overall_test_loss = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            f1_sum = 0\n",
    "            f1_sum_chunk = 0\n",
    "            f1_sum_tag = 0\n",
    "            for embeddings, labels in test_loader:\n",
    "                outputs = best_model(embeddings)\n",
    "                \n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                final_outputs = []\n",
    "                final_labels = []\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i]!=-1:\n",
    "                        max_index = torch.argmax(outputs[i])\n",
    "                        final_outputs.append(idx_to_bio[max_index.item()])\n",
    "                        final_labels.append(idx_to_bio[labels[i].item()])\n",
    "\n",
    "                formatted_data = format_for_conlleval(final_labels, final_outputs)\n",
    "\n",
    "                result = evaluate(formatted_data)\n",
    "\n",
    "                chunk_f1 = result['overall']['chunks']['evals']['f1']\n",
    "                tag_f1 = result['overall']['tags']['evals']['f1']\n",
    "                \n",
    "                f1_sum_chunk += chunk_f1\n",
    "                f1_sum_tag += tag_f1\n",
    "                f1_sum += (chunk_f1 + tag_f1)/2\n",
    "\n",
    "        print(f1_sum_chunk/len(test_loader))\n",
    "        print(f1_sum_tag/len(test_loader))\n",
    "        print(f1_sum/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271431f-86f7-4955-b59d-e979c99bf1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
